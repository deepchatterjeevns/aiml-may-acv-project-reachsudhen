{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FACE_RECOGNITION_Questions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "FZPrUBuA2D9n"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep face recognition with Keras"
      ]
    },
    {
      "metadata": {
        "id": "vH2C9eLwD8P2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "B3NdNn8BoS4y",
        "outputId": "01171c68-7e10-422a-bc32-a306d8ae278b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ayg3Af037u5j"
      },
      "cell_type": "markdown",
      "source": [
        "Set files_zip_path to the location in the drive where you have the new `'Files_required_for_face_recognition.zip'` file. This block will extract all the files to the current working directory. You should be seeing the list of all files inside the zip files as the output of this block after the final `!ls` command is executed. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5ObkEzXr66Sz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed73dfc4-ce31-4a14-9ce0-3b3b606cbdc8"
      },
      "cell_type": "code",
      "source": [
        "files_zip_path = \"/content/drive/My Drive/Colab Notebooks/Face Recognition/images.zip\"\n",
        "\n",
        "import zipfile\n",
        "#This extracts the files to the current working directory\n",
        "archive = zipfile.ZipFile(files_zip_path, 'r')\n",
        "archive.extractall()\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  images  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b_-jwcH7DaD9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Face Recognition/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YX7WhwXrEKj1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19ccc851-71b3-4022-83a4-6854195c4295"
      },
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/Face Recognition'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7jpaCy9Q1y4M"
      },
      "cell_type": "markdown",
      "source": [
        "### First, lets install the required libraries. Upload the `requirements.txt` file given and run the below commands."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "e9k1hVvWLnGw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3148
        },
        "outputId": "afcda42c-b860-4ec9-a004-6e87e7e39777"
      },
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt --user"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting absl-py==0.1.10 (from -r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/b8/3dafc45f20a817ab9f042302646bcbe6f7e26e8a760871a85637e53a35ec/absl-py-0.1.10.tar.gz (79kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 3.2MB/s \n",
            "\u001b[?25hCollecting appnope==0.1.0 (from -r requirements.txt (line 2))\n",
            "  Downloading https://files.pythonhosted.org/packages/87/a9/7985e6a53402f294c8f0e8eff3151a83f1fb901fa92909bb3ff29b4d22af/appnope-0.1.0-py2.py3-none-any.whl\n",
            "Collecting bleach==1.5.0 (from -r requirements.txt (line 3))\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.10.0)\n",
            "Collecting decorator==4.2.1 (from -r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/5a/53db15bf367d2028bdc6700dbdf1bdfab46b9f208b7516952817c0808118/decorator-4.2.1-py2.py3-none-any.whl\n",
            "Collecting entrypoints==0.2.3 (from -r requirements.txt (line 6))\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/8b/4eefa9b47f1910b3d2081da67726b066e379b04ca897acfe9f92bac56147/entrypoints-0.2.3-py2.py3-none-any.whl\n",
            "Collecting h5py==2.7.1 (from -r requirements.txt (line 7))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/b8/a63fcc840bba5c76e453dd712dbca63178a264c8990e0086b72965d4e954/h5py-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.4MB 8.3MB/s \n",
            "\u001b[?25hCollecting html5lib==0.9999999 (from -r requirements.txt (line 8))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K    100% |████████████████████████████████| 890kB 23.8MB/s \n",
            "\u001b[?25hCollecting jedi==0.11.1 (from -r requirements.txt (line 9))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ca/d71f5a427601c98eadabfd73104cacbec8cc230e8416158decf61a48b0c6/jedi-0.11.1-py2.py3-none-any.whl (250kB)\n",
            "\u001b[K    100% |████████████████████████████████| 256kB 29.8MB/s \n",
            "\u001b[?25hCollecting Jinja2==2.10 (from -r requirements.txt (line 10))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/ff/ae64bacdfc95f27a016a7bed8e8686763ba4d277a78ca76f32659220a731/Jinja2-2.10-py2.py3-none-any.whl (126kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema==2.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (2.6.0)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (1.0.0)\n",
            "Collecting jupyter-client==5.2.2 (from -r requirements.txt (line 13))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/8a/5cca4d98fe28c4db6c2879b69ca5d106e0f4415aafda8bc4074deddbc3da/jupyter_client-5.2.2-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 28.3MB/s \n",
            "\u001b[?25hCollecting jupyter-console==5.2.0 (from -r requirements.txt (line 14))\n",
            "  Downloading https://files.pythonhosted.org/packages/77/82/6469cd7fccf7958cbe5dce2e623f1e3c5e27f1bb1ad36d90519bc2d5d370/jupyter_console-5.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jupyter-core==4.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 15)) (4.4.0)\n",
            "Collecting Keras==2.1.3 (from -r requirements.txt (line 16))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/ae/7f94a03cb3f74cdc8a0f5f86d1df5c1dd686acb9a9c2a421c64f8497358e/Keras-2.1.3-py2.py3-none-any.whl (319kB)\n",
            "\u001b[K    100% |████████████████████████████████| 327kB 28.1MB/s \n",
            "\u001b[?25hCollecting Markdown==2.6.11 (from -r requirements.txt (line 17))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 27.8MB/s \n",
            "\u001b[?25hCollecting MarkupSafe==1.0 (from -r requirements.txt (line 18))\n",
            "  Downloading https://files.pythonhosted.org/packages/4d/de/32d741db316d8fdb7680822dd37001ef7a448255de9699ab4bfcbdf4172b/MarkupSafe-1.0.tar.gz\n",
            "Collecting matplotlib==2.1.2 (from -r requirements.txt (line 19))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/96/6e71a040339d4efc54cf707061e70fd624c9120b48b992f919ba34df9cb1/matplotlib-2.1.2-cp36-cp36m-manylinux1_x86_64.whl (15.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 15.0MB 3.4MB/s \n",
            "\u001b[?25hCollecting mistune==0.8.3 (from -r requirements.txt (line 20))\n",
            "  Downloading https://files.pythonhosted.org/packages/c8/8c/87f4d359438ba0321a2ae91936030110bfcc62fef752656321a72b8c1af9/mistune-0.8.3-py2.py3-none-any.whl\n",
            "Collecting nbconvert==5.3.1 (from -r requirements.txt (line 21))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/ea/280d6c0d92f8e3ca15fd798bbcc2ea141489f9539de7133d8fe10ea4b049/nbconvert-5.3.1-py2.py3-none-any.whl (387kB)\n",
            "\u001b[K    100% |████████████████████████████████| 389kB 14.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: nbformat==4.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 22)) (4.4.0)\n",
            "Collecting notebook==5.4.0 (from -r requirements.txt (line 23))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/ca/a5a5ac9c839868ceddaa2672c399492ac1dbd4c4ce68f833a72a619fc225/notebook-5.4.0-py2.py3-none-any.whl (8.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 8.0MB 5.7MB/s \n",
            "\u001b[?25hCollecting numpy==1.14.0 (from -r requirements.txt (line 24))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/ac/5c270dffb864f23315e9c1f9e0a0b300c797b3c170666c031c4de42aacae/numpy-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (17.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 17.2MB 2.7MB/s \n",
            "\u001b[?25hCollecting opencv-python==3.4.0.12 (from -r requirements.txt (line 25))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/f9/5c454f0f52788a913979877e6ed9b2454a9c7676581a0ee3a2d81db784a6/opencv_python-3.4.0.12-cp36-cp36m-manylinux1_x86_64.whl (24.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 24.9MB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandocfilters==1.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 26)) (1.4.2)\n",
            "Collecting parso==0.1.1 (from -r requirements.txt (line 27))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/2f/96f54499c920070ccc1bffaee115a6a0cf1a0e7ece34b8faa7ee632688dd/parso-0.1.1-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K    100% |████████████████████████████████| 92kB 26.6MB/s \n",
            "\u001b[?25hCollecting pexpect==4.3.1 (from -r requirements.txt (line 28))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/d1/1503ce5e3c41690333949cb887cf569933ef8835aa30af7e3b1b07fbd867/pexpect-4.3.1-py2.py3-none-any.whl (55kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 24.4MB/s \n",
            "\u001b[?25hCollecting pickleshare==0.7.4 (from -r requirements.txt (line 29))\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/17/daa142fc9be6b76f26f24eeeb9a138940671490b91cb5587393f297c8317/pickleshare-0.7.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: prompt-toolkit==1.0.15 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 30)) (1.0.15)\n",
            "Collecting protobuf==3.5.1 (from -r requirements.txt (line 31))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/99/471fa05dab1cf69419c91bcd7b5a7f3a0e251c76025bbdb40e08c367b728/protobuf-3.5.1-cp36-cp36m-manylinux1_x86_64.whl (6.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 6.4MB 8.1MB/s \n",
            "\u001b[?25hCollecting ptyprocess==0.5.2 (from -r requirements.txt (line 32))\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/4e/fa4a73ccfefe2b37d7b6898329e7dbcd1ac846ba3a3b26b294a78a3eb997/ptyprocess-0.5.2-py2.py3-none-any.whl\n",
            "Collecting Pygments==2.2.0 (from -r requirements.txt (line 33))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/ee/b6e02dc6529e82b75bb06823ff7d005b141037cb1416b10c6f00fc419dca/Pygments-2.2.0-py2.py3-none-any.whl (841kB)\n",
            "\u001b[K    100% |████████████████████████████████| 849kB 24.0MB/s \n",
            "\u001b[?25hCollecting pyparsing==2.2.0 (from -r requirements.txt (line 34))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/8a/718fd7d3458f9fab8e67186b00abdd345b639976bc7fb3ae722e1b026a50/pyparsing-2.2.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 24.8MB/s \n",
            "\u001b[?25hCollecting python-dateutil==2.6.1 (from -r requirements.txt (line 35))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/0d/7ed381ab4fe80b8ebf34411d14f253e1cf3e56e2820ffa1d8844b23859a2/python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)\n",
            "\u001b[K    100% |████████████████████████████████| 194kB 29.9MB/s \n",
            "\u001b[?25hCollecting pytz==2017.3 (from -r requirements.txt (line 36))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/7f/e7d1acbd433b929168a4fb4182a2ff3c33653717195a26c1de099ad1ef29/pytz-2017.3-py2.py3-none-any.whl (511kB)\n",
            "\u001b[K    100% |████████████████████████████████| 512kB 18.5MB/s \n",
            "\u001b[?25hCollecting PyYAML==3.12 (from -r requirements.txt (line 37))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/85/db5a2df477072b2902b0eb892feb37d88ac635d36245a72a6a69b23b383a/PyYAML-3.12.tar.gz (253kB)\n",
            "\u001b[K    100% |████████████████████████████████| 256kB 22.7MB/s \n",
            "\u001b[?25hCollecting pyzmq==16.0.4 (from -r requirements.txt (line 38))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/89/8d9118fc7b69290561d820064d92f5899f2e57fd235378a98307f080d8dc/pyzmq-16.0.4-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.1MB 9.0MB/s \n",
            "\u001b[?25hCollecting qtconsole==4.3.1 (from -r requirements.txt (line 39))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ff/047e0dca2627b162866920e7aa93f04523c0ae81e5c67060eec85701992d/qtconsole-4.3.1-py2.py3-none-any.whl (108kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 33.7MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.19.1 (from -r requirements.txt (line 40))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/2d/9fbc7baa5f44bc9e88ffb7ed32721b879bfa416573e85031e16f52569bc9/scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.4MB 4.1MB/s \n",
            "\u001b[?25hCollecting scipy==1.0.0 (from -r requirements.txt (line 41))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/5e/caa01ba7be11600b6a9d39265440d7b3be3d69206da887c42bef049521f2/scipy-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (50.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 50.0MB 1.0MB/s \n",
            "\u001b[?25hCollecting Send2Trash==1.4.2 (from -r requirements.txt (line 42))\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/de/17f0fd21a9d6565b7bf32e4747486bf13603252451887b93b4346c2d2690/Send2Trash-1.4.2-py3-none-any.whl\n",
            "Requirement already satisfied: simplegeneric==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 43)) (0.8.1)\n",
            "Requirement already satisfied: six==1.11.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 44)) (1.11.0)\n",
            "Collecting tensorflow==1.5.0 (from -r requirements.txt (line 45))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/79/a37d0b373757b4d283c674a64127bd8864d69f881c639b1ee5953e2d9301/tensorflow-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (44.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 44.4MB 1.0MB/s \n",
            "\u001b[?25hCollecting tensorflow-tensorboard==1.5.1 (from -r requirements.txt (line 46))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/fa/91c06952517b4f1bc075545b062a4112e30cebe558a6b962816cb33efa27/tensorflow_tensorboard-1.5.1-py3-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 12.8MB/s \n",
            "\u001b[?25hCollecting terminado==0.8.1 (from -r requirements.txt (line 47))\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/20/a26211a24425923d46e1213b376a6ee60dc30bcdf1b0c345e2c3769deb1c/terminado-0.8.1-py2.py3-none-any.whl\n",
            "Collecting testpath==0.3.1 (from -r requirements.txt (line 48))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/19/d7bc380c479a184e4a5a9ce516e4e2a773165f89b445f7cdced83d94de25/testpath-0.3.1-py2.py3-none-any.whl (161kB)\n",
            "\u001b[K    100% |████████████████████████████████| 163kB 32.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado==4.5.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 49)) (4.5.3)\n",
            "Requirement already satisfied: traitlets==4.3.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 50)) (4.3.2)\n",
            "Requirement already satisfied: wcwidth==0.1.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 51)) (0.1.7)\n",
            "Collecting Werkzeug==0.14.1 (from -r requirements.txt (line 52))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n",
            "\u001b[K    100% |████████████████████████████████| 327kB 23.6MB/s \n",
            "\u001b[?25hCollecting widgetsnbextension==3.1.3 (from -r requirements.txt (line 53))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/5a/8098ee2f0f990ccff22c97babcdc9c8a211434ed78887c321e8c483856bb/widgetsnbextension-3.1.3-py2.py3-none-any.whl (2.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.2MB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 12)) (7.4.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 12)) (4.6.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console==5.2.0->-r requirements.txt (line 14)) (5.5.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat==4.4.0->-r requirements.txt (line 22)) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf==3.5.1->-r requirements.txt (line 31)) (40.9.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5.0->-r requirements.txt (line 45)) (0.33.1)\n",
            "Building wheels for collected packages: absl-py, html5lib, MarkupSafe, PyYAML\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/34/67/31/e47fa35f2ece9669183e961e85479cc0b86de258b192ca1f2f\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "  Building wheel for MarkupSafe (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/33/56/20/ebe49a5c612fffe1c5a632146b16596f9e64676768661e4e46\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/03/05/65/bdc14f2c6e09e82ae3e0f13d021e1b6b2481437ea2f207df3f\n",
            "Successfully built absl-py html5lib MarkupSafe PyYAML\n",
            "\u001b[31myellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mtfds-nightly 1.0.2.dev201904090105 has requirement protobuf>=3.6.1, but you'll have protobuf 3.5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mtextgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 2.1.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mtensorflow-metadata 0.13.0 has requirement protobuf<4,>=3.7, but you'll have protobuf 3.5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mtensorboard 1.13.1 has requirement absl-py>=0.4, but you'll have absl-py 0.1.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mtensorboard 1.13.1 has requirement protobuf>=3.6.0, but you'll have protobuf 3.5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mspacy 2.0.18 has requirement numpy>=1.15.0, but you'll have numpy 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mnetworkx 2.2 has requirement decorator>=4.3.0, but you'll have decorator 4.2.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mmagenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.5.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mipywidgets 7.4.2 has requirement widgetsnbextension~=3.4.0, but you'll have widgetsnbextension 3.1.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mimgaug 0.2.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mimbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogleapis-common-protos 1.5.9 has requirement protobuf>=3.6.0, but you'll have protobuf 3.5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mgoogle-colab 1.0.0 has requirement notebook~=5.2.0, but you'll have notebook 5.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.51 has requirement numpy>=1.15, but you'll have numpy 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mdopamine-rl 1.0.5 has requirement absl-py>=0.2.2, but you'll have absl-py 0.1.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mdopamine-rl 1.0.5 has requirement opencv-python>=3.4.1.15, but you'll have opencv-python 3.4.0.12 which is incompatible.\u001b[0m\n",
            "\u001b[31mdatascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mcvxpy 1.0.15 has requirement scipy>=1.1.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "Installing collected packages: absl-py, appnope, html5lib, bleach, decorator, entrypoints, numpy, h5py, parso, jedi, MarkupSafe, Jinja2, python-dateutil, pyzmq, jupyter-client, Pygments, jupyter-console, PyYAML, scipy, Keras, Markdown, pytz, pyparsing, matplotlib, mistune, testpath, nbconvert, Send2Trash, ptyprocess, terminado, notebook, opencv-python, pexpect, pickleshare, protobuf, qtconsole, scikit-learn, Werkzeug, tensorflow-tensorboard, tensorflow, widgetsnbextension\n",
            "\u001b[33m  The scripts jupyter-kernel, jupyter-kernelspec and jupyter-run are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  The script pygmentize is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  The script jupyter-console is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  The script markdown_py is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  The script jupyter-nbconvert is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  The scripts jupyter-bundlerextension, jupyter-nbextension, jupyter-notebook and jupyter-serverextension are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  The script tensorboard is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  The scripts freeze_graph, saved_model_cli, tensorboard, toco and toco_from_protos are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed Jinja2-2.10 Keras-2.1.3 Markdown-2.6.11 MarkupSafe-1.0 PyYAML-3.12 Pygments-2.2.0 Send2Trash-1.4.2 Werkzeug-0.14.1 absl-py-0.1.10 appnope-0.1.0 bleach-1.5.0 decorator-4.2.1 entrypoints-0.2.3 h5py-2.7.1 html5lib-0.9999999 jedi-0.11.1 jupyter-client-5.2.2 jupyter-console-5.2.0 matplotlib-2.1.2 mistune-0.8.3 nbconvert-5.3.1 notebook-5.4.0 numpy-1.14.0 opencv-python-3.4.0.12 parso-0.1.1 pexpect-4.3.1 pickleshare-0.7.4 protobuf-3.5.1 ptyprocess-0.5.2 pyparsing-2.2.0 python-dateutil-2.6.1 pytz-2017.3 pyzmq-16.0.4 qtconsole-4.3.1 scikit-learn-0.19.1 scipy-1.0.0 tensorflow-1.5.0 tensorflow-tensorboard-1.5.1 terminado-0.8.1 testpath-0.3.1 widgetsnbextension-3.1.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "decorator",
                  "google",
                  "jupyter_client",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pexpect",
                  "pickleshare",
                  "pygments",
                  "pyparsing",
                  "pytz",
                  "zmq"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N0YcTuo3MmBS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "1f6d023e-013e-48b7-a3f3-2e2ee053a869"
      },
      "cell_type": "code",
      "source": [
        "!pip install request"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting request\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/27/7cbde262d854aedf217061a97020d66a63163c5c04e0ec02ff98c5d8f44e/request-2019.4.13.tar.gz\n",
            "Collecting get (from request)\n",
            "  Downloading https://files.pythonhosted.org/packages/3f/ef/bb46f77f7220ac1b7edba0c76d810c89fddb24ddd8c08f337b9b4a618db7/get-2019.4.13.tar.gz\n",
            "Collecting post (from request)\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/05/bd79da5849ea6a92485ed7029ef97b1b75e55c26bc0ed3a7ec769af666f3/post-2019.4.13.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from request) (40.9.0)\n",
            "Collecting query_string (from get->request)\n",
            "  Downloading https://files.pythonhosted.org/packages/12/3c/412a45daf5bea9b1d06d7de41787ec4168001dfa418db7ec8723356b119f/query-string-2019.4.13.tar.gz\n",
            "Collecting public (from query_string->get->request)\n",
            "  Downloading https://files.pythonhosted.org/packages/54/4d/b40004cc6c07665e48af22cfe1e631f219bf4282e15fa76a5b6364f6885c/public-2019.4.13.tar.gz\n",
            "Building wheels for collected packages: request, get, post, query-string, public\n",
            "  Building wheel for request (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/30/84/5f/484cfba678967ef58c16fce6890925d5c7172622f20111fbfd\n",
            "  Building wheel for get (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c1/e3/c1/d02c8c58538853e4c9b78cadb74f6d5c5c370b48a69a7271aa\n",
            "  Building wheel for post (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c3/c3/24/b5c132b537ab380c02d69e6bd4dec1f5db56b5fe19030473d7\n",
            "  Building wheel for query-string (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d6/a4/78/01b20a9dc224dcc009fab669f7f27b943b8889c5150bd68d8a\n",
            "  Building wheel for public (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/23/7c/6e/f5b4e09d6596c8b8802b347e48f149031e2363368048f1347a\n",
            "Successfully built request get post query-string public\n",
            "Installing collected packages: public, query-string, get, post, request\n",
            "Successfully installed get-2019.4.13 post-2019.4.13 public-2019.4.13 query-string-2019.4.13 request-2019.4.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bNbjzAgh3em-"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the network\n",
        "\n",
        "The CNN model is taken from the Keras-OpenFace project. The architecture details aren't too important here, it's only useful to know that there is a fully connected layer with 128 hidden units followed by an L2 normalization layer on top of the convolutional base. These two top layers are referred to as the embedding layer from which the 128-dimensional embedding vectors can be obtained. The complete model is defined in `model.py` and a graphical overview is given in `model.png`. A Keras version of the `nn4.small2` model can be created with `create_model()`.\n",
        "\n",
        "\n",
        "**Run the below code to initialize the model**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "cRMdAbSsMcPC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "b00ad2b8-c624-4ac4-dfee-16bc95292594"
      },
      "cell_type": "code",
      "source": [
        "from model import create_model\n",
        "#Disable tensorflow backend warnings\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import warnings\n",
        "# Suppress LabelEncoder warning\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "nn4_small2 = create_model()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "XyCaIac9N8_8"
      },
      "cell_type": "markdown",
      "source": [
        "#### Idea of Training the model with Triplet loss function "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fNKudbMe4u7W"
      },
      "cell_type": "markdown",
      "source": [
        "Model training aims to learn an embedding f(x) of image x such that the squared L2 distance between all faces of the same identity is small and the distance between a pair of faces from different identities is large. This can be achieved with a triplet loss L that is minimized when the distance between an anchor image xai and a positive image xpi (same identity) in embedding space is smaller than the distance between that anchor image and a negative image xni (different identity) by at least a margin α."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2Ose4tTyPDeU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Layer\n",
        "\n",
        "# Input for anchor, positive and negative images\n",
        "in_a = Input(shape=(96, 96, 3))\n",
        "in_p = Input(shape=(96, 96, 3))\n",
        "in_n = Input(shape=(96, 96, 3))\n",
        "\n",
        "# Output for anchor, positive and negative embedding vectors\n",
        "# The nn4_small model instance is shared (Siamese network)\n",
        "emb_a = nn4_small2(in_a)\n",
        "emb_p = nn4_small2(in_p)\n",
        "emb_n = nn4_small2(in_n)\n",
        "\n",
        "class TripletLossLayer(Layer):\n",
        "    def __init__(self, alpha, **kwargs):\n",
        "        self.alpha = alpha\n",
        "        super(TripletLossLayer, self).__init__(**kwargs)\n",
        "    \n",
        "    def triplet_loss(self, inputs):\n",
        "        a, p, n = inputs\n",
        "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
        "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
        "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        loss = self.triplet_loss(inputs)\n",
        "        self.add_loss(loss)\n",
        "        return loss\n",
        "\n",
        "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
        "triplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n",
        "\n",
        "# Model that can be trained with anchor, positive negative images\n",
        "nn4_small2_train = Model([in_a, in_p, in_n], triplet_loss_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qpTO6eimP5t0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "738ee189-d876-4e70-eb7b-2eee0390a10b"
      },
      "cell_type": "code",
      "source": [
        "from data import triplet_generator\n",
        "\n",
        "# triplet_generator() creates a generator that continuously returns \n",
        "# ([a_batch, p_batch, n_batch], None) tuples where a_batch, p_batch \n",
        "# and n_batch are batches of anchor, positive and negative RGB images \n",
        "# each having a shape of (batch_size, 96, 96, 3).\n",
        "generator = triplet_generator() \n",
        "\n",
        "nn4_small2_train.compile(loss=None, optimizer='adam')\n",
        "nn4_small2_train.fit_generator(generator, epochs=10, steps_per_epoch=100)\n",
        "\n",
        "# Please note that the current implementation of the generator only generates \n",
        "# random image data. The main goal of this code snippet is to demonstrate \n",
        "# the general setup for model training. In the following, we will anyway \n",
        "# use a pre-trained model so we don't need a generator here that operates \n",
        "# on real training data."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 35s 348ms/step - loss: 0.8105\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.8012\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 17s 172ms/step - loss: 0.8011\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 17s 170ms/step - loss: 0.7941\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 17s 171ms/step - loss: 0.8179\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 17s 171ms/step - loss: 0.8002\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 17s 169ms/step - loss: 0.8001\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 17s 173ms/step - loss: 0.8001\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 17s 175ms/step - loss: 0.8000\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 17s 174ms/step - loss: 0.8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f014af10208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AdsuNBe9OHyZ"
      },
      "cell_type": "markdown",
      "source": [
        "For this project, we are considering a pre-trained model given in file path **`nn4.small2.v1.h5`**.\n",
        "\n",
        "# 1. Using **load_weights()** function load the given pre-trained weight file. **- 10 Marks**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oV73nKP3QCLT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "bulEElwQ54g6"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "re9AUK4f520z"
      },
      "cell_type": "markdown",
      "source": [
        "To demonstrate face recognition on a custom dataset, a small dataset is used. It consists of around 15-25 face images of 10 different persons. The metadata for each image (file and identity name) are loaded into memory for later processing.\n",
        "\n",
        "\n",
        "Upload Images zip file given to drive and download and extract it using the below code. And we will pass the folder `images` to `load_metadata` function to save all the images filenames and person numbers."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sbDurWRMQnw7"
      },
      "cell_type": "markdown",
      "source": [
        "#### Import drive module from google.colab"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KYr8qfMD7Poc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0GlBCA8wQ1cX"
      },
      "cell_type": "markdown",
      "source": [
        "#### Give a path to mount the files in your drive"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZxhxrGQS7Pq2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/My Drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CBB_OncAQ8h_"
      },
      "cell_type": "markdown",
      "source": [
        "#### Using the above given mounted path, give the images.zip path dependent on where you placed the file in your drive."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pprDxdHT7S6N",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## For example\n",
        "images_path = \"./images.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "edsF05iuRkOd"
      },
      "cell_type": "markdown",
      "source": [
        "#### Using ZipFile module to extract the images zip file"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "m3eIglFdVcvB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CVAjzG4IXYQX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with ZipFile(images_path, 'r') as zip:\n",
        "  zip.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oesXJD9ySB6w"
      },
      "cell_type": "markdown",
      "source": [
        "#### Run the below function to load the images from the extracted images folder from the above step and map each image with person id \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4Q7TS19vVbGb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os.path\n",
        "\n",
        "class IdentityMetadata():\n",
        "    def __init__(self, base, name, file):\n",
        "        # print(base, name, file)\n",
        "        # dataset base directory\n",
        "        self.base = base\n",
        "        # identity name\n",
        "        self.name = name\n",
        "        # image file name\n",
        "        self.file = file\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.image_path()\n",
        "\n",
        "    def image_path(self):\n",
        "        return os.path.join(self.base, self.name, self.file) \n",
        "    \n",
        "def load_metadata(path):\n",
        "    metadata = []\n",
        "    for i in os.listdir(path):\n",
        "        for f in os.listdir(os.path.join(path, i)):\n",
        "            # Check file extension. Allow only jpg/jpeg' files.\n",
        "            ext = os.path.splitext(f)[1]\n",
        "            if ext == '.jpg' or ext == '.jpeg':\n",
        "                metadata.append(IdentityMetadata(path, i, f))\n",
        "    return np.array(metadata)\n",
        "\n",
        "metadata = load_metadata('images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uDV5yBp93Y-0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(metadata[0].file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Vr4xVNqIaHgE"
      },
      "cell_type": "markdown",
      "source": [
        "### Face Crop \n",
        "The nn4.small2.v1 model was trained with cropped face images, therefore, the face images from the custom dataset must be cropped too. Here, we use the face detection model we built in Milestone 2 and use it to detect faces and crop the faces to pass to our model."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JbWhL7jbUwHg"
      },
      "cell_type": "markdown",
      "source": [
        "##### 1. Import the pre-trained model from milestone 2. You would need all the python files required for the face-detection assignment and the trained weights as well\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3FKiloh93Y_H",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from mn_model import mn_model\n",
        "from face_generator import BatchGenerator\n",
        "from keras_ssd_loss import SSDLoss\n",
        "from ssd_box_encode_decode_utils import SSDBoxEncoder, decode_y, decode_y2\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "\n",
        "img_height =512\n",
        "img_width = 512\n",
        "\n",
        "img_channels = 3\n",
        "\n",
        "n_classes =2 \n",
        "class_names = [\"background\",\"face\"]\n",
        "\n",
        "scales = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # anchorboxes for coco dataset\n",
        "aspect_ratios = [[0.5, 1.0, 2.0],\n",
        "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
        "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
        "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
        "                 [0.5, 1.0, 2.0],\n",
        "                 [0.5, 1.0, 2.0]] # The anchor box aspect ratios used in the original SSD300\n",
        "two_boxes_for_ar1 = True\n",
        "limit_boxes = True # Whether or not you want to limit the anchor boxes to lie entirely within the image boundaries\n",
        "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are scaled as in the original implementation\n",
        "coords = 'centroids' # Whether the box coordinates to be used as targets for the model should be in the 'centroids' or 'minmax' format, see documentation\n",
        "normalize_coords = True\n",
        "\n",
        "model, model_layer, img_input, predictor_sizes = mn_model(image_size=(img_height, img_width, img_channels), \n",
        "                                                                      n_classes = n_classes,\n",
        "                                                                      min_scale = None, \n",
        "                                                                      max_scale = None, \n",
        "                                                                      scales = scales, \n",
        "                                                                      aspect_ratios_global = None, \n",
        "                                                                      aspect_ratios_per_layer = aspect_ratios, \n",
        "                                                                      two_boxes_for_ar1= two_boxes_for_ar1, \n",
        "                                                                      limit_boxes=limit_boxes, \n",
        "                                                                      variances= variances, \n",
        "                                                                      coords=coords, \n",
        "                                                                      normalize_coords=normalize_coords)\n",
        "\n",
        "#model.summary()\n",
        "model_path = './'\n",
        "model_name = 'ssd_mobilenet_face_epoch_25_loss0.0916.h5'\n",
        "\n",
        "model.load_weights(model_path + model_name,  by_name= True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pGNgNidM3Y_T"
      },
      "cell_type": "markdown",
      "source": [
        "##### 2. Function to load an image and reverse the channels for matplot lib to show them\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ape5WxvVWKOe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def load_image(path):\n",
        "    img = cv2.imread(path)\n",
        "\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "L0rtSevW3Y_0"
      },
      "cell_type": "markdown",
      "source": [
        "You can access each image path from `metadata[i].image_path()` where, i is the image number. We can take values from 1 to no.of images in the dataset given and plot them using matplotlib's imshow."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ptDNq8noWK89",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load an image\n",
        "# for example, loading the image with index 1\n",
        "one_image = load_image(metadata[1].image_path())\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Show original image\n",
        "# OpenCV loads images with color channels\n",
        "# in BGR order. So we need to reverse them everytime we use matplotlib to show the image.\n",
        "# Otherwise you see the image in false colour\n",
        "plt.imshow(one_image)[...,::-1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vbIw3kKY3ZAL"
      },
      "cell_type": "markdown",
      "source": [
        "##### 3. A function to get the face bounding box x_min, x_max, y_min and y_max of input image using the face detection model given an image read using cv2.imread.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ILEa4E6R3ZAM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def face_bb(test_img):\n",
        "    img_height =512\n",
        "    img_width = 512\n",
        "    img_channels = 3\n",
        "    _CONF = 0.60\n",
        "    _IOU = 0.15\n",
        "    coords = 'centroids' # Whether the box coordinates to be used as targets for the model should be in the 'centroids' or 'minmax' format, see documentation\n",
        "    normalize_coords = True\n",
        "    org_height = test_img.shape[0]\n",
        "    org_width = test_img.shape[1]\n",
        "    test_img = cv2.resize(test_img, (512, 512))\n",
        "    test_img_input = np.expand_dims(test_img,axis=0)\n",
        "    y_pred = model.predict(test_img_input)\n",
        "    y_pred_decoded = decode_y2(y_pred,\n",
        "                                 confidence_thresh=_CONF,\n",
        "                                iou_threshold=_IOU,\n",
        "                                top_k='all',\n",
        "                                input_coords=coords,\n",
        "                                normalize_coords=normalize_coords,\n",
        "                                img_height=img_height,\n",
        "                                img_width=img_width)\n",
        "    result = y_pred_decoded[0][0]\n",
        "    det_label = result[0]\n",
        "    det_conf = result[1]\n",
        "    det_xmin = result[2]\n",
        "    det_xmax = result[3]\n",
        "    det_ymin = result[4]\n",
        "    det_ymax = result[5]\n",
        "    #Converting to integers as the indexes to images are only integers\n",
        "    bb = [int(det_xmin),int(det_xmax),int(det_ymin),int(det_ymax)]\n",
        "    return bb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1OkENkFW3ZAZ"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Write a function to get the resized cropped out face of an input image from the path given using the face_bb() function defined above. - 10 Marks\n",
        "Hint: The face_bb() function gives a list of bounding box co-ordinate values like **[x_min,x_max,y_min,y_max]**. Use **cv2.imread()** to load an image from the image path and pass it to the face_bb function as input. You can crop an image by simply accessing the specific values of the image matrix using the co-ordinates as the boundary indexes. But, before cropping we have to resize the input image to (512,512) like done in the **face_bb()** function as the bounding box co-ordinates are for the resized image. A simple crop can be implemented on the resized image by doing **\"image[y_min:y_max,x_min:x_max,:]\"** . \n",
        "You have to then resize the cropped image to a fixed size of (96,96). You can do that using the **cv2.resize()** function again. It takes the second argument as (96,96) this time. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "54fWVBxn3ZAb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_face(img_path):\n",
        "    \n",
        "    ###\n",
        "    ### WRITE YOUR CODE HERE\n",
        "    ###\n",
        "\n",
        "    return crop_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fpe2G6JSbTis"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Write code to load 2nd and 3rd images in the metadata using load_image() and show each image and its cropped version side by side for comparison using matplotlib imshow - 20 Marks"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vEEzp1SkbdxI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EQ3VYQDW3ZA1",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LkBQRL_sd2U8"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate embeddings for each image in the dataset\n",
        "\n",
        "Given below is an example to load the first image in the metadata and get its embedding vector from the pre-trained model. "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "S01r8UzXc-8s"
      },
      "cell_type": "markdown",
      "source": [
        "#### Get embedding vector for first image in the metadata using the pre-trained model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "B2yd69OydBAq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Align the image\n",
        "cropped_img = get_face(metadata[0].image_path())\n",
        "\n",
        "# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\n",
        "img = (cropped_img / 255.).astype(np.float32)\n",
        "\n",
        "# obtain embedding vector for an image\n",
        "embedding_vector = nn4_small2_pretrained.predict(np.expand_dims(img, axis=0))[0]\n",
        "\n",
        "print(embedding_vector.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "plHvUTytcTGo"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. Write code to iterate through metadata and create embeddings for each image using nn4_small2_pretrained.predict() and store in a list with name `embeddings` - 10 marks\n",
        "\n",
        "If there is any error in reading any image in the dataset, fill the emebdding vector of that image with 128-zeroes as the final embedding from the model is of length 128.\n",
        "Hint: Don't forget to use numpy's expand_dims funtion to convert each image into the shape (1,96,96,3) and take the 0th element of the prediction from the model as the embedding of the image. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yY9ykxtueY4k",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4hb3XSDsfTMG"
      },
      "cell_type": "markdown",
      "source": [
        "#### Write code to get the distance between the respective embeddings given 2 pairs of images.\n",
        "\n",
        "Consider distance metric as \"Squared L2 distance\"\n",
        "\n",
        "squared l2 distance between 2 points (x1, y1) and (x2, y2) = (x1-x2)^2 + (y1-y2)^2\n",
        "\n",
        "\n",
        "\n",
        "# 5. Plot images and get distance between the pairs given below. - 10 Marks\n",
        "\n",
        "1. 10,12 and 10,131\n",
        "\n",
        "2. 30,31 and 30,100\n",
        "\n",
        "3. 70,72 and 70,115"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nDVLED10eboB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "N_HfgsQXgV6u"
      },
      "cell_type": "markdown",
      "source": [
        "#### Now lets build a simple fully connected neural network classifier to predict person in the given image. "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AahHoLK1hZJj"
      },
      "cell_type": "markdown",
      "source": [
        "If you observe distances between more pairs the difference is not constant between opposite pairs. So we train a fully connected neural network with very limited dataset to classiffy each embedding into the person. Lets prepare the data for training a fully connected neural network using keras"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2gfLzoBIhrpV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "targets = np.array([int(m.name) for m in metadata])\n",
        "\n",
        "num_classes = 11\n",
        "# convert class vectors to one-hot encodings\n",
        "y = keras.utils.to_categorical(targets, num_classes)\n",
        "\n",
        "\n",
        "train_idx = np.arange(metadata.shape[0]) % 2 != 0\n",
        "test_idx = np.arange(metadata.shape[0]) % 2 == 0\n",
        "\n",
        "## checking the shapes of metaadata and test and train sets\n",
        "print(metadata.shape)\n",
        "print(train_idx.shape)\n",
        "print(test_idx.shape)\n",
        "\n",
        "\n",
        "# one half as train examples of 10 identities\n",
        "X_train = embeddings[train_idx]\n",
        "X_test = embeddings[test_idx]\n",
        "\n",
        "\n",
        "y_train = y[train_idx]\n",
        "y_test = y[test_idx]\n",
        "\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "# svc = LinearSVC()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "312NcUeuiNpL"
      },
      "cell_type": "markdown",
      "source": [
        "# 6. Build a 1 layered fully connected neural network using keras and report the accuracy - 20 marks\n",
        "Hint: Create only one layer with 11 neurons as we have 11 classes and mention the input_dim as 128 as the embeddings are of 128 length. Use a small batch size like 4 and use softmax as the activation function of the layer. \n",
        "You might have to train upto 80 epochs to get decent accuracy. You can use an Adam optimizer for the training part"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6ybglrDph7tJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JqFfYIZbiXG8"
      },
      "cell_type": "markdown",
      "source": [
        "# 7.  Test the classifier  - 10 marks\n",
        "\n",
        "Take 35th image from test set and plot the image, report to which person(folder name in dataset) the image belongs to."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "p2E6WBuMiBmX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CmfxVcBH3ZBp"
      },
      "cell_type": "markdown",
      "source": [
        "## Optional Exercise - For people who are familiar with SVMs, you can use an SVM classifier(If you already know how to use the scikit-learn package for it) to do this last part being done by the fully connected neural network."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2xCUexkj3ZBq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "targets = np.array([m.name for m in metadata])\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(targets)\n",
        "\n",
        "# Numerical encoding of identities\n",
        "y = encoder.transform(targets)\n",
        "\n",
        "train_idx = np.arange(metadata.shape[0]) % 2 != 0\n",
        "test_idx = np.arange(metadata.shape[0]) % 2 == 0\n",
        "\n",
        "## checking the shapes of metaadata and test and train sets\n",
        "print(metadata.shape)\n",
        "print(train_idx.shape)\n",
        "print(test_idx.shape)\n",
        "\n",
        "\n",
        "# one half as train examples of 10 identities\n",
        "X_train = embeddings[train_idx]\n",
        "# another half as test examples of 10 identities\n",
        "X_test = embeddings[test_idx]\n",
        "\n",
        "y_train = y[train_idx]\n",
        "y_test = y[test_idx]\n",
        "\n",
        "svc = LinearSVC()\n",
        "\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "acc_svc = accuracy_score(y_test, svc.predict(X_test))\n",
        "\n",
        "print(f'SVM accuracy = {acc_svc}')\n",
        "\n",
        "import warnings\n",
        "# Suppress LabelEncoder warning\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "example_idx = 44\n",
        "\n",
        "example_image = load_image(metadata[test_idx][example_idx].image_path())\n",
        "example_prediction = svc.predict([embeddings[test_idx][example_idx]])\n",
        "example_identity = encoder.inverse_transform(example_prediction)[0]\n",
        "\n",
        "plt.imshow(example_image)\n",
        "plt.title(f'Identified as {example_identity}');"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}